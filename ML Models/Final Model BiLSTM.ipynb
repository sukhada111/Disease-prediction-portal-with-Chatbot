{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800000 800000\n",
      "266666 266666\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "533332"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_raw=pd.read_csv('../Dataset/training_tweets.csv',encoding = \"ISO-8859-1\", header=None)\n",
    "\n",
    " # As the data has no column titles, we will add our own\n",
    "df_raw.columns = [\"label\", \"time\", \"date\", \"query\", \"username\", \"text\"]\n",
    "\n",
    "df = df_raw[['label', 'text']]\n",
    "df_pos = df[df['label'] == 4]\n",
    "df_neg = df[df['label'] == 0]\n",
    "print(len(df_pos), len(df_neg))\n",
    "\n",
    "df_pos = df_pos.iloc[:int(len(df_pos)/3)]\n",
    "df_neg = df_neg.iloc[:int(len(df_neg)/3)]\n",
    "print(len(df_pos), len(df_neg))\n",
    "\n",
    "# Concatenating both positive and negative groups and storing them back into a single dataframe\n",
    "df = pd.concat([df_pos, df_neg])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "\n",
    "# The reduce_len parameter will allow a maximum of 3 consecutive repeating characters, while trimming the rest\n",
    "# For example, it will tranform the word: 'Helloooooooooo' to: 'Hellooo'\n",
    "tk = TweetTokenizer(reduce_len=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# Separating our features (text) and our labels into two lists to smoothen our work\n",
    "X = df['text'].tolist()\n",
    "Y = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Time: 62.40374684333801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(['I', 'LOVE', '@Health4UandPets', 'u', 'guys', 'r', 'the', 'best', '!', '!'],\n",
       "  1),\n",
       " (['im',\n",
       "   'meeting',\n",
       "   'up',\n",
       "   'with',\n",
       "   'one',\n",
       "   'of',\n",
       "   'my',\n",
       "   'besties',\n",
       "   'tonight',\n",
       "   '!',\n",
       "   'Cant',\n",
       "   'wait',\n",
       "   '!',\n",
       "   '!',\n",
       "   '-',\n",
       "   'GIRL',\n",
       "   'TALK',\n",
       "   '!',\n",
       "   '!'],\n",
       "  1),\n",
       " (['@DaRealSunisaKim',\n",
       "   'Thanks',\n",
       "   'for',\n",
       "   'the',\n",
       "   'Twitter',\n",
       "   'add',\n",
       "   ',',\n",
       "   'Sunisa',\n",
       "   '!',\n",
       "   'I',\n",
       "   'got',\n",
       "   'to',\n",
       "   'meet',\n",
       "   'you',\n",
       "   'once',\n",
       "   'at',\n",
       "   'a',\n",
       "   'HIN',\n",
       "   'show',\n",
       "   'here',\n",
       "   'in',\n",
       "   'the',\n",
       "   'DC',\n",
       "   'area',\n",
       "   'and',\n",
       "   'you',\n",
       "   'were',\n",
       "   'a',\n",
       "   'sweetheart',\n",
       "   '.'],\n",
       "  1),\n",
       " (['Being',\n",
       "   'sick',\n",
       "   'can',\n",
       "   'be',\n",
       "   'really',\n",
       "   'cheap',\n",
       "   'when',\n",
       "   'it',\n",
       "   'hurts',\n",
       "   'too',\n",
       "   'much',\n",
       "   'to',\n",
       "   'eat',\n",
       "   'real',\n",
       "   'food',\n",
       "   'Plus',\n",
       "   ',',\n",
       "   'your',\n",
       "   'friends',\n",
       "   'make',\n",
       "   'you',\n",
       "   'soup'],\n",
       "  1),\n",
       " (['@LovesBrooklyn2', 'he', 'has', 'that', 'effect', 'on', 'everyone'], 1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Building our data list, that is a list of tuples, where each tuple is a pair of the tokenized text\n",
    "# and its corresponding label\n",
    "for x, y in zip(X, Y):\n",
    "    if y == 4:\n",
    "        data.append((tk.tokenize(x), 1))\n",
    "    else:\n",
    "        data.append((tk.tokenize(x), 0))\n",
    "        \n",
    "# Printing the CPU time and the first 5 elements of our 'data' list\n",
    "print('CPU Time:', time() - start_time)\n",
    "data[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Stopwords are frequently-used words (such as “the”, “a”, “an”, “in”) that do not hold any meaning useful to extract sentiment.\n",
    "# If it's your first time ever using nltk, you can download nltk's stopwords using: nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "# Defining a handy function in order to load a given glove file\n",
    "\n",
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map\n",
    "\n",
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')\n",
    "\n",
    "from tensorflow.keras.models import Sequential, load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Defining a function that will initialize and populate our embedding layer\n",
    "\n",
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"unk\"].shape[0] #50\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "        \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False, input_shape=(max_len,))\n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further data cleaning\n",
    "# A custom function defined in order to fine-tune the cleaning of the input text.\n",
    "# This function is being \"upgraded\" such that it performs a more thourough cleaning of the data\n",
    "# in order to better fit our words embedding layer\n",
    "def cleaned(token):\n",
    "    if token == 'u':\n",
    "        return 'you'\n",
    "    if token == 'r':\n",
    "        return 'are'\n",
    "    if token == 'some1':\n",
    "        return 'someone'\n",
    "    if token == 'yrs':\n",
    "        return 'years'\n",
    "    if token == 'hrs':\n",
    "        return 'hours'\n",
    "    if token == 'mins':\n",
    "        return 'minutes'\n",
    "    if token == 'secs':\n",
    "        return 'seconds'\n",
    "    if token == 'pls' or token == 'plz':\n",
    "        return 'please'\n",
    "    if token == '2morow' or token == '2moro' or token=='tmrw' or token=='tomorow':\n",
    "        return 'tomorrow'\n",
    "    if token == '2day':\n",
    "        return 'today'\n",
    "    if token == '4got' or token == '4gotten':\n",
    "        return 'forget'\n",
    "    if token in ['hahah', 'hahaha', 'hahahaha', 'hehehe', 'hahahah','hahahahaha']:\n",
    "        return 'haha'\n",
    "    if token == \"mother's\":\n",
    "        return \"mother\"\n",
    "    if token == \"mom's\":\n",
    "        return \"mom\"\n",
    "    if token == \"dad's\":\n",
    "        return \"dad\"\n",
    "    if token == 'bday' or token == 'b-day':\n",
    "        return 'birthday'\n",
    "    if token in [\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\", \"hadn't\",\"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"shouldn't\", \"ain't\",\"weren't\", \"should've\", \"would've\",\"could've\" ,\"here's\",\"where's\"]:\n",
    "        return token.replace(\"'\", \"\")\n",
    "    if token in ['lmao', 'lolz', 'rofl']:\n",
    "        return 'lol'\n",
    "    if token == '<3':\n",
    "        return 'love'\n",
    "    if token == 'thanx' or token == 'thnx':\n",
    "        return 'thanks'\n",
    "    if token == 'goood':\n",
    "        return 'good'\n",
    "    if token in ['amp', 'quot', 'lt', 'gt', '½25', '..', '. .', '. . .']:\n",
    "        return ''     \n",
    "    if token == 'awsome' or token=='awsm':\n",
    "        return 'awesome'\n",
    "    if token in [\"g'night\",\"gn\",\"gooodnight\"]:\n",
    "        return 'goodnight'\n",
    "    if token == '#fb':\n",
    "        return 'fb'\n",
    "    if token in ['proly','prolly']:\n",
    "        return 'probably'\n",
    "    if token in ['omfg','omgg']:\n",
    "        return 'omg'\n",
    "\n",
    "    return token\n",
    "\n",
    "\n",
    "# This function will be our all-in-one noise removal function\n",
    "def remove_noise(tweet_tokens):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token in tweet_tokens:\n",
    "        # Eliminating the token if it is a link\n",
    "        token = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", token)\n",
    "        # Eliminating the token if it is a mention\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "        \n",
    "        cleaned_token = cleaned(token.lower())\n",
    "        \n",
    "        if cleaned_token == \"idk\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('dont')\n",
    "            cleaned_tokens.append('know')\n",
    "            continue\n",
    "        if cleaned_token == \"i'll\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"you'll\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"we'll\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"it'll\":\n",
    "            cleaned_tokens.append('it')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        #added\n",
    "        if cleaned_token == \"they'll\" or cleaned_token== \"they'l\":\n",
    "            cleaned_tokens.append('they')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"he'll\" or cleaned_token== \"he'l\":\n",
    "            cleaned_tokens.append('he')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        if cleaned_token == \"she'll\" or cleaned_token== \"she'l\":\n",
    "            cleaned_tokens.append('she')\n",
    "            cleaned_tokens.append('will')\n",
    "            continue\n",
    "        \n",
    "        if cleaned_token == \"it's\":\n",
    "            cleaned_tokens.append('it')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"i've\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"you've\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"we've\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"they've\":\n",
    "            cleaned_tokens.append('they')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"you're\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"we're\":\n",
    "            cleaned_tokens.append('we')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"they're\":\n",
    "            cleaned_tokens.append('they')\n",
    "            cleaned_tokens.append('are')\n",
    "            continue\n",
    "        if cleaned_token == \"let's\":\n",
    "            cleaned_tokens.append('let')\n",
    "            cleaned_tokens.append('us')\n",
    "            continue\n",
    "        if cleaned_token == \"she's\":\n",
    "            cleaned_tokens.append('she')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"he's\":\n",
    "            cleaned_tokens.append('he')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"that's\":\n",
    "            cleaned_tokens.append('that')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"i'd\":\n",
    "            cleaned_tokens.append('i')\n",
    "            cleaned_tokens.append('would')\n",
    "            continue\n",
    "        if cleaned_token == \"you'd\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('would')\n",
    "            continue\n",
    "        if cleaned_token == \"there's\":\n",
    "            cleaned_tokens.append('there')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"what's\":\n",
    "            cleaned_tokens.append('what')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"how's\":\n",
    "            cleaned_tokens.append('how')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"who's\":\n",
    "            cleaned_tokens.append('who')\n",
    "            cleaned_tokens.append('is')\n",
    "            continue\n",
    "        if cleaned_token == \"y'all\" or cleaned_token == \"ya'll\":\n",
    "            cleaned_tokens.append('you')\n",
    "            cleaned_tokens.append('all')\n",
    "            continue\n",
    "        #added by me\n",
    "        if cleaned_token == \"hadnt\":\n",
    "            cleaned_tokens.append('had')\n",
    "            cleaned_tokens.append('not')\n",
    "            continue\n",
    "        if cleaned_token == \"shouldnt\":\n",
    "            cleaned_tokens.append('should')\n",
    "            cleaned_tokens.append('not')\n",
    "            continue\n",
    "        if cleaned_token == \"werent\":\n",
    "            cleaned_tokens.append('were')\n",
    "            cleaned_tokens.append('not')\n",
    "            continue\n",
    "        if cleaned_token == \"shouldve\":\n",
    "            cleaned_tokens.append('should')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token == \"wouldve\":\n",
    "            cleaned_tokens.append('would')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "        if cleaned_token==\"tbh\":\n",
    "            cleaned_tokens.append('to')\n",
    "            cleaned_tokens.append('be')\n",
    "            cleaned_tokens.append('honest')\n",
    "            continue\n",
    "        if cleaned_token == \"couldve\":\n",
    "            cleaned_tokens.append('could')\n",
    "            cleaned_tokens.append('have')\n",
    "            continue\n",
    "       \n",
    "        if cleaned_token.strip() and cleaned_token not in string.punctuation:\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "            \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Prevewing the remove_noise() output\n",
    "# print(remove_noise(data[0][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time()\n",
    "\n",
    "unks = []\n",
    "UNKS = []\n",
    "\n",
    "def cleared(word):\n",
    "    res = \"\"\n",
    "    prev = None\n",
    "    for char in word:\n",
    "        if char == prev: continue\n",
    "        prev = char\n",
    "        res += char\n",
    "    return res\n",
    "\n",
    "def sentence_to_indices(sentence_words, word_to_index, max_len, i):\n",
    "    global X, Y\n",
    "    sentence_indices = []\n",
    "    for j, w in enumerate(sentence_words):\n",
    "        try:\n",
    "            index = word_to_index[w]\n",
    "        except:\n",
    "            UNKS.append(w)\n",
    "            w = cleared(w)\n",
    "            try:\n",
    "                index = word_to_index[w]\n",
    "            except:\n",
    "                index = word_to_index['unk']\n",
    "                unks.append(w)\n",
    "        X[i, j] = index\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed Noise, CPU Time: 50.908814668655396\n",
      "max_len: 162\n",
      "Data Prepared for model, CPU Time: 3.566823720932007\n",
      "[[185457. 226278. 394475. 169754.  58997. 357266.  74390.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      " [187631. 239792. 373317. 388711. 269953. 268046. 254258. 372306. 361859.\n",
      "   91041. 382320. 161844. 352214.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      " [357161. 151349. 357266. 368306.  46173. 372306. 185457. 164934. 360915.\n",
      "  239785. 394475. 269889.  62065.  43010. 179025. 329974. 177231. 188481.\n",
      "  357266. 118061.  58999.  54718. 394475. 385664.  43010. 349437.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      " [ 72182. 330826.  90548.  71090. 302352.  97698. 386424. 193716. 184338.\n",
      "  361940. 251645. 360915. 132701. 302292. 151204. 286963. 394565. 154060.\n",
      "  231458. 394475. 338210.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      " [175199. 174032. 357212. 133896. 269798. 141948.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "       0.      0.      0.      0.      0.      0.      0.      0.      0.]]\n",
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cleaned_tokens_list = []\n",
    "\n",
    "# Removing noise from all the data, using the newly defined function\n",
    "for tokens, label in data:\n",
    "    x = remove_noise(tokens)\n",
    "    if x:\n",
    "        cleaned_tokens_list.append((x, label))\n",
    "\n",
    "print('Removed Noise, CPU Time:', time() - start_time)\n",
    "start_time = time()\n",
    "\n",
    "list_len = [len(i) for i, j in cleaned_tokens_list]\n",
    "max_len = max(list_len)\n",
    "print('max_len:', max_len)\n",
    "\n",
    "\n",
    "X = np.zeros((len(cleaned_tokens_list), max_len))\n",
    "Y = np.zeros((len(cleaned_tokens_list), ))\n",
    "\n",
    "\n",
    "for i, tk_lb in enumerate(cleaned_tokens_list):\n",
    "    tokens, label = tk_lb\n",
    "    sentence_to_indices(tokens, word_to_index, max_len, i)\n",
    "    Y[i] = label\n",
    "    \n",
    "print('Data Prepared for model, CPU Time:', time() - start_time)\n",
    "\n",
    "\n",
    "print(X[:5])\n",
    "print(Y[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('#folowfriday', 1500),\n",
       " (':/', 928),\n",
       " ('(:', 604),\n",
       " ('. .', 511),\n",
       " ('tweps', 469),\n",
       " (\":'(\", 446),\n",
       " (';-)', 396),\n",
       " ('->', 365),\n",
       " ('awh', 364),\n",
       " (\"today's\", 363),\n",
       " ('iï', 359),\n",
       " ('2morow', 350),\n",
       " ('d:', 348),\n",
       " ('#asot40', 333),\n",
       " ('urgh', 271),\n",
       " ('ahaha', 266),\n",
       " ('<-', 258),\n",
       " ('sux', 250),\n",
       " ('8:', 236),\n",
       " ('yey', 234),\n",
       " ('retwet', 232),\n",
       " ('bleh', 219),\n",
       " ('probs', 214),\n",
       " (\"friend's\", 199),\n",
       " ('twiterverse', 194),\n",
       " (\"everyone's\", 192),\n",
       " ('damnit', 191),\n",
       " ('woho', 185),\n",
       " ('=/', 180),\n",
       " (':-d', 176),\n",
       " ('itï', 175),\n",
       " ('=]', 168),\n",
       " ('#delongeday', 167),\n",
       " ('bestie', 164),\n",
       " ('lï', 160),\n",
       " ('twiterland', 159),\n",
       " ('twiterbery', 151),\n",
       " ('everyones', 151),\n",
       " ('hayfever', 151),\n",
       " ('cï', 148),\n",
       " (':\\\\', 147),\n",
       " (\"it'd\", 146),\n",
       " ('mï', 143),\n",
       " ('):', 138),\n",
       " (\"that'l\", 138),\n",
       " ('ahah', 137),\n",
       " (\"night's\", 135),\n",
       " ('nï', 133),\n",
       " ('folowfriday', 132),\n",
       " ('xox', 131)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unk = word_to_index['unk']\n",
    "\n",
    "n_unk_words = 0\n",
    "\n",
    "for x in X:\n",
    "    for y in x:\n",
    "        if y == unk:\n",
    "            n_unk_words += 1\n",
    "\n",
    "print(n_unk_words)\n",
    "\n",
    "from collections import Counter\n",
    "Counter(unks).most_common(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitted\n",
      "426665\n",
      "106667\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0, stratify=Y)\n",
    "print(\"splitted\")\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(X_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "185457"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 309, 50)           20000050  \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 309, 256)          183296    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 20,577,843\n",
      "Trainable params: 577,793\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      " 13/624 [..............................] - ETA: 6:06:21 - loss: 0.6832 - accuracy: 0.5557"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m model_clean_data\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#500 works\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[43mmodel_clean_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m model_clean_data\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBiLSTM_final.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:108\u001b[0m, in \u001b[0;36menable_multi_worker.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_method_wrapper\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    107\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_multi_worker_mode():  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m   \u001b[38;5;66;03m# Running inside `run_distribute_coordinator` already.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m dc_context\u001b[38;5;241m.\u001b[39mget_current_worker_context():\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1098\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1091\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1092\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraceContext\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1093\u001b[0m     graph_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1094\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1095\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1096\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size):\n\u001b[0;32m   1097\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1098\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1100\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:780\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    779\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 780\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    782\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tracing_count()\n\u001b[0;32m    783\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:807\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    804\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    805\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    806\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 807\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    809\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    810\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    811\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2829\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2828\u001b[0m   graph_function, args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filtered_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1843\u001b[0m, in \u001b[0;36mConcreteFunction._filtered_call\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_filtered_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, args, kwargs, cancellation_manager\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1828\u001b[0m   \u001b[38;5;124;03m\"\"\"Executes the function, filtering arguments from the Python function.\u001b[39;00m\n\u001b[0;32m   1829\u001b[0m \n\u001b[0;32m   1830\u001b[0m \u001b[38;5;124;03m  Objects aside from Tensors, CompositeTensors, and Variables are ignored.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1841\u001b[0m \u001b[38;5;124;03m    `args` and `kwargs`.\u001b[39;00m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1843\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[43m      \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpand_composites\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1846\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mresource_variable_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBaseResourceVariable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1847\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[43m      \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1923\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1918\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1919\u001b[0m     pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeSetPossibleGradientTypes(args))\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m _POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1921\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1922\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1923\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1924\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1925\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1926\u001b[0m     args,\n\u001b[0;32m   1927\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1928\u001b[0m     executing_eagerly)\n\u001b[0;32m   1929\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:545\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    544\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 545\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    552\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    553\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    554\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    558\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf-cuda-env\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "tensorflow.keras.backend.clear_session()\n",
    "# gpu_devices = tensorflow.config.experimental.list_physical_devices(\"GPU\")\n",
    "# for device in gpu_devices:\n",
    "#     tensorflow.config.experimental.set_memory_growth(device, True)\n",
    "\n",
    "model_clean_data = Sequential()\n",
    "\n",
    "model_clean_data.add(pretrained_embedding_layer(word_to_vec_map, word_to_index, max_len))\n",
    "model_clean_data.add(Bidirectional(LSTM(units=128, return_sequences=True)))\n",
    "model_clean_data.add(Bidirectional(LSTM(units=128, return_sequences=False)))\n",
    "model_clean_data.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model_clean_data.summary()\n",
    "\n",
    "model_clean_data.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "#500 works\n",
    "model_clean_data.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs = 5, batch_size = 512, shuffle=True)\n",
    "model_clean_data.save(\"BiLSTM.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_clean_data.history\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training Accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "y_arrow = max(val_acc)\n",
    "x_arrow = val_acc.index(y_arrow) + 1\n",
    "plt.annotate(str(y_arrow)[:6],\n",
    "             (x_arrow, y_arrow),\n",
    "             xytext=(x_arrow + 5, y_arrow + .02),\n",
    "             arrowprops=dict(facecolor='orange', shrink=0.05))\n",
    "plt.xticks(epochs)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.xticks(epochs)\n",
    "plt.show()\n",
    "\n",
    "model_clean_data.save(\"BiLSTM_4.h5\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "built_model=tensorflow.keras.models.load_model('BiLSTM_tune_1_rerun.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_indices(sentence_words, max_len):\n",
    "    X = np.zeros((max_len))\n",
    "    sentence_indices = []\n",
    "    for j, w in enumerate(sentence_words):\n",
    "        try:\n",
    "            index = word_to_index[w]\n",
    "        except:\n",
    "            w = cleared(w)\n",
    "            try:\n",
    "                index = word_to_index[w]\n",
    "            except:\n",
    "                index = word_to_index['unk']\n",
    "        X[j] = index\n",
    "    return X\n",
    "\n",
    "def predict_sentiment(custom_tweet):\n",
    "    # Convert the tweet such that it can be fed to the model\n",
    "    x_input = sentence_to_indices(remove_noise(tk.tokenize(custom_tweet)), 162) #max_len=162 for our model final\n",
    "    print(len(x_input))\n",
    "    \n",
    "    # Retrun the model's prediction\n",
    "    return round(built_model.predict(np.array([x_input])).item(),3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.011\n",
      "<class 'numpy.ndarray'>\n",
      "0.99\n",
      "<class 'numpy.ndarray'>\n",
      "0.006\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I'm not happy you're here\"))\n",
    "\n",
    "print(predict_sentiment(\"I'm glad you're here!\"))\n",
    "\n",
    "print(predict_sentiment(\"I'm sad you're here!\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n",
      "0.123\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" I have been through a lot of things which have affected me\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185457.  52943. 146352.  43010. 223830.  52315. 217901.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "0.076\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" I am feeling a little alone lately\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[185457. 388583. 185457. 110156. 270501. 373317.  44608. 193716.  54718.\n",
      " 352214. 360915. 337267.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "0.058\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" I wish I could open up about it and talk to someone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[357640. 192973.  43010. 225985. 163745. 269798.  62065. 389836.  87775.\n",
      " 185457. 168566. 185457.  90548. 172590. 193716.  88126. 254554.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "0.336\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" There is a lot going on at work but I guess I can handle it by myself\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[357640. 192973.  43010. 225985. 163745. 269798.  62065. 389836. 386474.\n",
      " 185457.  52943. 264550.  44493. 360915. 108724. 388711.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.\n",
      "      0.      0.      0.      0.      0.      0.      0.      0.      0.]\n",
      "0.057\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" There is a lot going on at work which I am not able to cope with\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.745\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" I'm quite happy with my family , they support me when I am down\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.997\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" I am so glad to have such supportive friends\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.893\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\" I received a promotion yesterday!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"my dog tommy died\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.192\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"the team i support lost the tournament\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.548\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"Oh come on it is not a bad thing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"my mental health is not that bad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.487\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I can’t go nowhere tonight\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.284\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"The feeling he experienced is not insignificant\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"with this act, it will be his first and probably, the last movie.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.412\n"
     ]
    }
   ],
   "source": [
    "print(predict_sentiment(\"I am not that unhappy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im\n",
      "187631\n",
      "dont\n",
      "127708\n",
      "cant\n",
      "91041\n",
      "couldnt\n",
      "110159\n",
      "arent\n",
      "59057\n",
      "wouldnt\n",
      "390144\n",
      "isnt\n",
      "193408\n",
      "didnt\n",
      "123557\n",
      "doesnt\n",
      "126852\n",
      "wont\n",
      "389498\n",
      "havent\n",
      "174666\n",
      "wasnt\n",
      "383633\n",
      "hasnt\n",
      "174194\n",
      "aint\n",
      "49032\n",
      "wouldve\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'wouldve'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m i\u001b[38;5;241m=\u001b[39mi\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mword_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'wouldve'"
     ]
    }
   ],
   "source": [
    "\n",
    "l1=[\"i'm\", \"don't\", \"can't\", \"couldn't\", \"aren't\", \"wouldn't\", \"isn't\", \"didn't\",\n",
    "                 \"doesn't\", \"won't\", \"haven't\", \"wasn't\", \"hasn't\", \"ain't\", \"would've\"]\n",
    "\n",
    "for i in l1:\n",
    "   \n",
    "    i=i.replace(\"'\", \"\")\n",
    "    print(i)\n",
    "    print(word_to_index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hadnt, shouldnt, werent, shouldve, wouldve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#folowfriday'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1={'social_anx':\n",
    "           [\"oh no\", \"that's bad\"],\n",
    "       'social_pos':\n",
    "           ['damn','sad','horrible']\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "horrible\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(dict1['depression']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-cuda-env",
   "language": "python",
   "name": "tf-cuda-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
